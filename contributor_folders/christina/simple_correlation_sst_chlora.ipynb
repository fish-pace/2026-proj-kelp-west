{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99b56d-cfd7-46a6-9bae-00e5c32ca737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing sst baseline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec487125c894e7d941e64bb3d49439a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16795bc98f9d4d30923fa056efdf9b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8200f2d908ee4259b3c1398b8125010e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import earthaccess\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import rioxarray # Required for swath-to-grid matching\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "# setup and authentication\n",
    "auth = earthaccess.login()\n",
    "bounds = {\"min_lon\": -120.75, \"max_lon\": -119, \"min_lat\": 33.5, \"max_lat\": 34.5}\n",
    "date_range = (\"2024-04-01\", \"2024-12-31\")\n",
    "\n",
    "# target grid\n",
    "target_lat = np.arange(bounds['min_lat'], bounds['max_lat'], 0.01)\n",
    "target_lon = np.arange(bounds['min_lon'], bounds['max_lon'], 0.01)\n",
    "target_grid = xr.Dataset(coords={'lat': target_lat, 'lon': target_lon})\n",
    "\n",
    "# sst baseline (MUR L4 is already gridded, so standard interp works)\n",
    "print(\"processing sst baseline...\")\n",
    "sst_query = earthaccess.search_data(\n",
    "    short_name=\"MUR-JPL-L4-GLOB-v4.1\",\n",
    "    bounding_box=(bounds['min_lon'], bounds['min_lat'], bounds['max_lon'], bounds['max_lat']),\n",
    "    temporal=date_range\n",
    ")\n",
    "ds_sst = xr.open_mfdataset(earthaccess.open(sst_query), chunks={'time': 10}, decode_timedelta=True)\n",
    "\n",
    "sst_monthly = (ds_sst.analysed_sst - 273.15).sel(\n",
    "    lat=slice(bounds['min_lat'], bounds['max_lat']),\n",
    "    lon=slice(bounds['min_lon'], bounds['max_lon'])\n",
    ").interp_like(target_grid, method=\"linear\").resample(time=\"1MS\").mean().compute()\n",
    "\n",
    "# agg pace data\n",
    "def process_pace_monthly(short_name, var_name, group_name):\n",
    "    print(f\"searching for {short_name}...\")\n",
    "    query = earthaccess.search_data(\n",
    "        short_name=short_name,\n",
    "        bounding_box=(bounds['min_lon'], bounds['min_lat'], bounds['max_lon'], bounds['max_lat']),\n",
    "        temporal=date_range\n",
    "    )\n",
    "    files = earthaccess.open(query)\n",
    "    storage = {}\n",
    "\n",
    "    for f in files:\n",
    "        try:\n",
    "            nav = xr.open_dataset(f, group=\"navigation_data\", engine=\"h5netcdf\")\n",
    "            data = xr.open_dataset(f, group=group_name, engine=\"h5netcdf\")\n",
    "            \n",
    "            with xr.open_dataset(f, engine=\"h5netcdf\") as meta:\n",
    "                t = pd.to_datetime(meta.attrs['time_coverage_start'])\n",
    "                m_key = t.strftime('%Y-%m')\n",
    "\n",
    "            if m_key not in storage:\n",
    "                storage[m_key] = [\n",
    "                    np.zeros((len(target_lat), len(target_lon))), \n",
    "                    np.zeros((len(target_lat), len(target_lon)))\n",
    "                ]\n",
    "\n",
    "            # NASA Fix Part 1: Prepare swath with rioxarray\n",
    "            # We assign coordinates and define the spatial dims for the rioxarray engine\n",
    "            subset = data[var_name].assign_coords({\n",
    "                \"lat\": ((\"number_of_lines\", \"pixels_per_line\"), nav.latitude.values),\n",
    "                \"lon\": ((\"number_of_lines\", \"pixels_per_line\"), nav.longitude.values)\n",
    "            })\n",
    "            subset = subset.rio.set_spatial_dims(\"pixels_per_line\", \"number_of_lines\")\n",
    "            subset = subset.rio.write_crs(\"epsg:4326\")\n",
    "\n",
    "            # NASA Fix Part 2: Reproject using the geolocation array\n",
    "            # This warps the 2D swath into your 1D target_lat/target_lon grid\n",
    "            regrid = subset.rio.reproject(\n",
    "                dst_crs=\"epsg:4326\",\n",
    "                shape=(len(target_lat), len(target_lon)),\n",
    "                src_geoloc_array=(subset[\"lon\"], subset[\"lat\"]),\n",
    "                resample=Resampling.nearest,\n",
    "                nodata=np.nan\n",
    "            ).compute()\n",
    "            \n",
    "            # The reprojected output uses 'x' and 'y', we ensure values are extracted correctly\n",
    "            valid_mask = ~np.isnan(regrid.values)\n",
    "            storage[m_key][0] += np.nan_to_num(regrid.values)\n",
    "            storage[m_key][1] += valid_mask.astype(float)\n",
    "            \n",
    "            del subset, regrid, nav, data\n",
    "            gc.collect()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"skipping granule: {e}\")\n",
    "\n",
    "    monthly_arrays = []\n",
    "    for m_key, (r_sum, r_count) in storage.items():\n",
    "        avg = np.divide(r_sum, r_count, out=np.full_like(r_sum, np.nan), where=r_count > 0)\n",
    "        da = xr.DataArray(avg, coords=[('lat', target_lat), ('lon', target_lon)], name=var_name)\n",
    "        da = da.expand_dims(time=[pd.to_datetime(m_key)])\n",
    "        monthly_arrays.append(da)\n",
    "        \n",
    "    if not monthly_arrays:\n",
    "        raise ValueError(f\"no data successfully processed for {short_name}\")\n",
    "        \n",
    "    return xr.concat(monthly_arrays, dim='time').sortby('time')\n",
    "\n",
    "# Execute Processing\n",
    "pace_chl_monthly = process_pace_monthly(\"PACE_OCI_L2_BGC\", \"chlor_a\", \"geophysical_data\")\n",
    "pace_bbp_monthly = process_pace_monthly(\"PACE_OCI_L2_IOP\", \"bbp_442\", \"geophysical_data\")\n",
    "\n",
    "# Saving Synthesis\n",
    "analysis_ds = xr.Dataset({\n",
    "    \"chlor_a\": pace_chl_monthly,\n",
    "    \"bbp_442\": pace_bbp_monthly,\n",
    "    \"sst\": sst_monthly\n",
    "})\n",
    "analysis_ds.to_netcdf(\"l2_sst_correlation.nc\")\n",
    "\n",
    "# Visualization Logic (Remains the same as your draft)\n",
    "print(\"Plotting results...\")\n",
    "# ... [Rest of your plotting code] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d1d38-a53a-46a0-9cf7-c6e92567f6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
